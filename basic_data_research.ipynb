{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6fb0e7f0",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e235368b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "from sklearn.impute import KNNImputer, SimpleImputer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, OneHotEncoder\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a90387f",
   "metadata": {},
   "source": [
    "Loading file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b445d006",
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.getcwd()  # Текущая рабочая директория\n",
    "\n",
    "file_path_read_train = os.path.join(current_dir, \"agentic_ai_performance_dataset_20250622.csv\")\n",
    "\n",
    "df = pd.read_csv(file_path_read_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6cfdc6",
   "metadata": {},
   "source": [
    "Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "84f37bd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_id                        object\n",
       "agent_type                      object\n",
       "model_architecture              object\n",
       "deployment_environment          object\n",
       "task_category                   object\n",
       "task_complexity                  int64\n",
       "autonomy_level                   int64\n",
       "success_rate                   float64\n",
       "accuracy_score                 float64\n",
       "efficiency_score               float64\n",
       "execution_time_seconds         float64\n",
       "response_latency_ms            float64\n",
       "memory_usage_mb                float64\n",
       "cpu_usage_percent              float64\n",
       "cost_per_task_cents            float64\n",
       "human_intervention_required       bool\n",
       "error_recovery_rate            float64\n",
       "multimodal_capability             bool\n",
       "edge_compatibility                bool\n",
       "privacy_compliance_score       float64\n",
       "bias_detection_score           float64\n",
       "timestamp                       object\n",
       "data_quality_score             float64\n",
       "performance_index              float64\n",
       "cost_efficiency_ratio          float64\n",
       "autonomous_capability_score    float64\n",
       "dtype: object"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78221b40",
   "metadata": {},
   "source": [
    "Estimated source data:\n",
    "\n",
    "agent_type\n",
    "\n",
    "model_architecture\n",
    "\n",
    "deployment_environment\n",
    "\n",
    "task_category\n",
    "\n",
    "task_complexity\n",
    "\n",
    "autonomy_level\n",
    "\n",
    "human_intervention_required\n",
    "\n",
    "multimodal_capability\n",
    "\n",
    "edge_compatibility\n",
    "\n",
    "timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad0734a",
   "metadata": {},
   "source": [
    "-------------------------------------\n",
    "Estimated result data:\n",
    "\n",
    "success_rate\n",
    "\n",
    "accuracy_score\n",
    "\n",
    "efficiency_score\n",
    "\n",
    "execution_time_seconds\n",
    "\n",
    "response_latency_ms\n",
    "\n",
    "memory_usage_mb\n",
    "\n",
    "cpu_usage_percent\n",
    "\n",
    "cost_per_task_cents\n",
    "\n",
    "error_recovery_rate\n",
    "\n",
    "privacy_compliance_score\n",
    "\n",
    "bias_detection_score\n",
    "\n",
    "data_quality_score (Maybe it's source data!)\n",
    "\n",
    "performance_index\n",
    "\n",
    "cost_efficiency_ratio\n",
    "\n",
    "autonomous_capability_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff18e973",
   "metadata": {},
   "source": [
    "----------------------------------------\n",
    "Agent Types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12143cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "agent_type\n",
       "Customer Service        340\n",
       "Project Manager         332\n",
       "Marketing Assistant     322\n",
       "Research Assistant      321\n",
       "Email Manager           320\n",
       "HR Recruiter            318\n",
       "Document Processor      317\n",
       "Social Media Manager    315\n",
       "Task Planner            314\n",
       "Financial Advisor       313\n",
       "Sales Assistant         313\n",
       "Content Creator         308\n",
       "Data Analyst            299\n",
       "QA Tester               296\n",
       "Translation Agent       292\n",
       "Code Assistant          280\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['agent_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4034c1",
   "metadata": {},
   "source": [
    "Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dbc9fa15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "model_architecture\n",
       "InstructGPT       540\n",
       "Transformer-XL    530\n",
       "Claude-3.5        512\n",
       "Falcon-180B       511\n",
       "Mixtral-8x7B      502\n",
       "GPT-4o            494\n",
       "PaLM-2            484\n",
       "Gemini-Pro        481\n",
       "LLaMA-3           479\n",
       "CodeT5+           467\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['model_architecture'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ccd6d3",
   "metadata": {},
   "source": [
    "Deployment Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a0968b1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deployment_environment\n",
       "Mobile     882\n",
       "Cloud      840\n",
       "Edge       836\n",
       "Server     835\n",
       "Hybrid     818\n",
       "Desktop    789\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['deployment_environment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a4ebd57",
   "metadata": {},
   "source": [
    "Task Category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "40fff4b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_category\n",
       "Communication               563\n",
       "Text Processing             528\n",
       "Problem Solving             523\n",
       "Data Analysis               512\n",
       "Learning & Adaptation       492\n",
       "Planning & Scheduling       489\n",
       "Code Generation             476\n",
       "Creative Writing            475\n",
       "Decision Making             471\n",
       "Research & Summarization    471\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['task_category'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a15c6",
   "metadata": {},
   "source": [
    "Task Complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88dbd6ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "task_complexity\n",
       "6     797\n",
       "7     766\n",
       "5     745\n",
       "4     642\n",
       "8     639\n",
       "3     445\n",
       "9     441\n",
       "10    314\n",
       "2     211\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['task_complexity'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0095308",
   "metadata": {},
   "source": [
    "Autonomy Level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ff265488",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "autonomy_level\n",
       "6     732\n",
       "7     712\n",
       "5     698\n",
       "4     593\n",
       "8     593\n",
       "10    468\n",
       "9     410\n",
       "3     377\n",
       "2     253\n",
       "1     164\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['autonomy_level'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ae282f",
   "metadata": {},
   "source": [
    "Human Intervention Required"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "63b2d65b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_intervention_required\n",
       "True     4393\n",
       "False     607\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['human_intervention_required'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94cd53e4",
   "metadata": {},
   "source": [
    "💡 What does multimodal capability mean?\n",
    "It refers to the AI's ability to work with multiple types of data simultaneously, such as:\n",
    "\n",
    "📷 Images\n",
    "\n",
    "📝 Text\n",
    "\n",
    "🔊 Audio\n",
    "\n",
    "📹 Video\n",
    "\n",
    "📊 Sensor or numerical data\n",
    "\n",
    "\n",
    "\n",
    "🔍 Examples:\n",
    "Model/Agent\tmultimodal_capability\n",
    "\n",
    "GPT-3\tFalse (text only)\n",
    "\n",
    "GPT-4V (Vision)\tTrue\n",
    "\n",
    "Gemini, Claude Opus\tTrue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d33fd50",
   "metadata": {},
   "source": [
    "Multimodal Capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "def9f0d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "multimodal_capability\n",
       "False    4260\n",
       "True      740\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['multimodal_capability'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1920650e",
   "metadata": {},
   "source": [
    "💡 What is Edge in the AI context?\n",
    "Edge computing means performing computation on or near the device (closer to the user), instead of sending all data to the cloud. Typical edge devices include:\n",
    "\n",
    "📱 Smartphones\n",
    "\n",
    "🤖 Robots\n",
    "\n",
    "🕹️ IoT devices\n",
    "\n",
    "🧠 Autonomous systems (e.g., cars, drones)\n",
    "\n",
    "Running models on edge improves latency, privacy, and sometimes cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea35cb0",
   "metadata": {},
   "source": [
    "Edge Compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e9eee921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "edge_compatibility\n",
       "True     2693\n",
       "False    2307\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['edge_compatibility'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5eefa2bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "success_rate\n",
       "0–0.25         0\n",
       "0.25–0.5    2790\n",
       "0.5–0.75    1879\n",
       "0.75–0.9     320\n",
       "0.9–1         11\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['success_rate'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc37270",
   "metadata": {},
   "source": [
    "Accuracy Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a5971c4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.4 Max: 0.9596\n"
     ]
    }
   ],
   "source": [
    "accuracy_score_min = df['accuracy_score'].min() \n",
    "accuracy_score_max = df['accuracy_score'].max()\n",
    "\n",
    "print(f\"Min: {accuracy_score_min} Max: {accuracy_score_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "7d5d0ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "accuracy_score\n",
       "0–0.25         0\n",
       "0.25–0.5    1698\n",
       "0.5–0.75    2807\n",
       "0.75–0.9     483\n",
       "0.9–1         12\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['accuracy_score'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd7a0b9",
   "metadata": {},
   "source": [
    "Efficience Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "54e07dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 0.3 Max: 0.8855\n"
     ]
    }
   ],
   "source": [
    "efficiency_score_min = df['efficiency_score'].min() \n",
    "efficiency_score_max = df['efficiency_score'].max()\n",
    "\n",
    "print(f\"Min: {efficiency_score_min} Max: {efficiency_score_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4f7911ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "efficiency_score\n",
       "0–0.25         0\n",
       "0.25–0.5    1228\n",
       "0.5–0.75    3321\n",
       "0.75–0.9     451\n",
       "0.9–1          0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['efficiency_score'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2903516",
   "metadata": {},
   "source": [
    "Privacy Compliance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3bc84b",
   "metadata": {},
   "source": [
    "------------------------------\n",
    "The field privacy_compliance_score represents:\n",
    "\n",
    "📊 A score indicating how well an AI agent complies with data privacy regulations and standards.\n",
    "\n",
    "💡 What does Privacy Compliance mean in the AI context?\n",
    "It refers to the agent's ability to:\n",
    "\n",
    "Follow legal and regulatory requirements (e.g., GDPR, HIPAA, CCPA)\n",
    "\n",
    "Avoid leaking personal or sensitive user data\n",
    "\n",
    "Limit data retention (don't store data longer than needed)\n",
    "\n",
    "Avoid unauthorized use of sensitive data\n",
    "\n",
    "Apply anonymization and access controls\n",
    "\n",
    "\n",
    "🔢 Score meaning:\n",
    "Usually ranges from 0 to 1 (or 0% to 100%)\n",
    "\n",
    "1.0 = Fully privacy-compliant\n",
    "\n",
    "0.0 = No privacy safeguards\n",
    "\n",
    "📌 Example use cases:\n",
    "\n",
    "Agent\tprivacy_compliance_score\tNotes\n",
    "\n",
    "GPT-4 API\t0.95\tDoesn't store conversations by default\n",
    "\n",
    "Custom bot\t0.40\tKeeps logs with user-identifiable data\n",
    "\n",
    "Edge agent\t1.00\tProcesses everything locally (no cloud)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "fbce35d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "privacy_compliance_score\n",
       "0–0.25         0\n",
       "0.25–0.5       0\n",
       "0.5–0.75     864\n",
       "0.75–0.9    2640\n",
       "0.9–1       1496\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['privacy_compliance_score'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e95a8b",
   "metadata": {},
   "source": [
    "Bias Detection Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca3ba620",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "The field bias_detection_score represents:\n",
    "\n",
    "⚖️ A score indicating how well the AI model detects or avoids bias in its responses or behavior.\n",
    "\n",
    "💡 What is bias in AI?\n",
    "Bias refers to systematic favoritism or discrimination in the model's behavior toward or against:\n",
    "\n",
    "specific groups (e.g., gender, race, age)\n",
    "\n",
    "political, cultural, or ideological stances\n",
    "\n",
    "topics, languages, or regions\n",
    "\n",
    "What does bias_detection_score measure?\n",
    "It is a metric that reflects how well the model:\n",
    "\n",
    "identifies potential bias in content\n",
    "\n",
    "avoids generating biased or harmful responses\n",
    "\n",
    "The values typically range from 0.0 to 1.0:\n",
    "\n",
    "1.0 — the model actively monitors for bias and avoids it\n",
    "\n",
    "0.0 — the model may produce biased or harmful outputs without checks\n",
    "\n",
    "🧪 How is it measured?\n",
    "It can be evaluated using:\n",
    "\n",
    "specific bias benchmarks (e.g., StereoSet, CrowS-Pairs)\n",
    "\n",
    "internal toxicity/fairness filters\n",
    "\n",
    "the presence of safety guardrails during response generation\n",
    "\n",
    "📌 Examples:\n",
    "\n",
    "Model\tbias_detection_score\tNotes\n",
    "\n",
    "GPT-4 (OpenAI)\t0.91\tStrong protections against gender/racial bias\n",
    "\n",
    "LLaMA (raw)\t0.55\tMay output offensive or biased content\n",
    "\n",
    "Claude AI (Anthropic)\t0.97\tHeavily focused on ethics, fairness, and safety"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3058dbc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bias_detection_score\n",
       "0–0.25         0\n",
       "0.25–0.5       0\n",
       "0.5–0.75    2175\n",
       "0.75–1      2825\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['bias_detection_score'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d104682d",
   "metadata": {},
   "source": [
    "Data Quality Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba31c6c",
   "metadata": {},
   "source": [
    "--------------------------------\n",
    "MUST CHECK!\n",
    "\n",
    "Is data_quality_score an input or output?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d9440a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data_quality_score\n",
       "0–0.25         0\n",
       "0.25–0.5       0\n",
       "0.5–0.75       1\n",
       "0.75–0.9    3287\n",
       "0.9–1       1712\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['data_quality_score'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd7e174",
   "metadata": {},
   "source": [
    "Performance Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f9dba812",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performance_index\n",
       "0–0.25         0\n",
       "0.25–0.5    2083\n",
       "0.5–0.75    2541\n",
       "0.75–0.9     376\n",
       "0.9–1          0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['performance_index'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "283a0ccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "performance_index\n",
       "0–0.25         0\n",
       "0.25–0.5    2083\n",
       "0.5–0.75    2541\n",
       "0.75–0.9     376\n",
       "0.9–1          0\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Определяем фиксированные интервалы\n",
    "bins = [0, 0.25, 0.5, 0.75, 0.9, 1.0]\n",
    "labels = ['0–0.25', '0.25–0.5', '0.5–0.75', '0.75–0.9', '0.9–1']\n",
    "\n",
    "# Разбиваем success_rate по этим интервалам\n",
    "quartile_bins = pd.cut(df['performance_index'], bins=bins, labels=labels, include_lowest=True, right=True)\n",
    "\n",
    "# Считаем количество значений в каждом интервале\n",
    "counts = quartile_bins.value_counts().sort_index()\n",
    "\n",
    "counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ccd94c",
   "metadata": {},
   "source": [
    "Cost Efficiency Ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7897d507",
   "metadata": {},
   "source": [
    "----------------------------\n",
    "The field cost_efficiency_ratio represents:\n",
    "\n",
    "💰📈 The ratio between an agent’s efficiency and its cost — essentially, how much useful performance you get per unit of cost.\n",
    "\n",
    "💡 What it means:\n",
    "This is a composite metric typically calculated as:\n",
    "\n",
    "cost_efficiency_ratio = efficiency_score / cost_per_task_cents\n",
    "\n",
    "It measures how cost-effective an agent is in performing its tasks.\n",
    "\n",
    "🔢 Interpreting the values:\n",
    "\n",
    "Higher values = better performance for less cost (more cost-efficient)\n",
    "\n",
    "Lower values = poor efficiency or high cost (less cost-effective)\n",
    "\n",
    "📌 Examples:\n",
    "\n",
    "Agent\tefficiency_score\tcost_per_task_cents\tcost_efficiency_ratio\tNotes\n",
    "\n",
    "GPT-4 Turbo\t0.85\t0.015\t56.7\tHigh efficiency per cost\n",
    "\n",
    "Legacy Model\t0.50\t0.025\t20.0\tSlower and more expensive\n",
    "\n",
    "Edge Agent\t0.65\t0.005\t130.0\tVery cheap, reasonable performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "fbf90593",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 5.951596638655461 Max: 219.327027027027\n"
     ]
    }
   ],
   "source": [
    "cost_efficiency_ratio_min = df['cost_efficiency_ratio'].min() \n",
    "cost_efficiency_ratio_max = df['cost_efficiency_ratio'].max()\n",
    "\n",
    "print(f\"Min: {cost_efficiency_ratio_min} Max: {cost_efficiency_ratio_max}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a267bd",
   "metadata": {},
   "source": [
    "Autonomous Capability Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "68f5b9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min: 56.284 Max: 147.69400000000002\n"
     ]
    }
   ],
   "source": [
    "autonomous_capability_score_min = df['autonomous_capability_score'].min() \n",
    "autonomous_capability_score_max = df['autonomous_capability_score'].max()\n",
    "\n",
    "print(f\"Min: {autonomous_capability_score_min} Max: {autonomous_capability_score_max}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "evgenie_python_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
